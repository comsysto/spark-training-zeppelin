{"paragraphs":[{"text":"%md <img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613987_495661109","id":"20190424-153346_1010921897","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:28093"},{"text":"%md\n# 1.5 Conversion between DataFrames, Datasets and RDDs","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1.5 Conversion between DataFrames, Datasets and RDDs</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613987_748324014","id":"20190424-153355_158323812","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28094"},{"text":"%md\n***1.5.1 Converting a DataFrame to a Dataset***\n\n***1.5.2 Converting a DataFrame to a RDD***\n\n***1.5.3 Converting a Dataset to a DataFrame***\n\n***1.5.4 Converting a Dataset to a RDD***\n\n***1.5.5 Converting a RDD to a DataFrame***\n\n***1.5.6 Converting a RDD to a Dataset***","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong><em>1.5.1 Converting a DataFrame to a Dataset</em></strong></p>\n<p><strong><em>1.5.2 Converting a DataFrame to a RDD</em></strong></p>\n<p><strong><em>1.5.3 Converting a Dataset to a DataFrame</em></strong></p>\n<p><strong><em>1.5.4 Converting a Dataset to a RDD</em></strong></p>\n<p><strong><em>1.5.5 Converting a RDD to a DataFrame</em></strong></p>\n<p><strong><em>1.5.6 Converting a RDD to a Dataset</em></strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613987_-930758080","id":"20190424-153404_1781432162","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28095"},{"text":"%md\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>","user":"anonymous","dateUpdated":"2019-05-17T10:36:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613987_99312941","id":"20190424-153513_359092411","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T10:36:26+0000","dateFinished":"2019-05-17T10:36:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28096"},{"text":"%spark\n// Path of data source file\nval dataPath = \"s3a://cs-spark-basic-training/Songs/\"","user":"anonymous","dateUpdated":"2019-05-17T11:33:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dataPath: String = s3a://cs-spark-basic-training/Songs/\n"}]},"apps":[],"jobName":"paragraph_1558088613988_-2008208888","id":"20190424-153414_1480740024","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T10:36:18+0000","dateFinished":"2019-05-17T10:36:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28097"},{"text":"%md\n<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' style='width:150px'>","user":"anonymous","dateUpdated":"2019-05-17T10:36:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' style='width:150px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613988_-1473854354","id":"20190424-153525_1825460273","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T10:36:21+0000","dateFinished":"2019-05-17T10:36:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28098"},{"text":"%pyspark\n# Path of data source file\ndataPath = \"s3a://cs-spark-basic-training/Songs/\"","user":"anonymous","dateUpdated":"2019-05-17T11:33:12+0000","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558088613988_1334588756","id":"20190424-153422_1370522374","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T10:38:08+0000","dateFinished":"2019-05-17T10:38:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28099"},{"text":"%md\n## 1.5.1 Converting a DataFrame to a Dataset","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5.1 Converting a DataFrame to a Dataset</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613988_179761392","id":"20190424-153536_1662214098","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28100"},{"text":"%md\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>","user":"anonymous","dateUpdated":"2019-05-17T10:36:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613988_1672855635","id":"20190424-154042_424807347","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T10:36:47+0000","dateFinished":"2019-05-17T10:36:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28101"},{"text":"%md\n* Using a case class","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Using a case class</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613989_894372945","id":"20190424-153544_520930718","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28102"},{"text":"%spark\n// Read in data as a DataFrame\nval songDF = spark.read.load(dataPath)","user":"anonymous","dateUpdated":"2019-05-17T11:20:12+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"songDF: org.apache.spark.sql.DataFrame = [artist: string, loudness: double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=96"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613989_555231294","id":"20190424-153553_708475123","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:20:12+0000","dateFinished":"2019-05-17T11:20:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28103"},{"text":"%spark\n// Definition of the case class 'Music' with two parameters which are representing the column names and their corresponding data types\ncase class Music(artist: String, loudness: Double)","user":"anonymous","dateUpdated":"2019-05-17T11:20:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","lineNumbers":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Music\n"}]},"apps":[],"jobName":"paragraph_1558091946206_-2112314357","id":"20190517-111906_1108750276","dateCreated":"2019-05-17T11:19:06+0000","dateStarted":"2019-05-17T11:20:15+0000","dateFinished":"2019-05-17T11:20:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28104"},{"text":"%spark\n// Conversion of the DataFrame 'songDF' into a Dataset by using the Dataset function 'as', which maps the columns of the DataFrame to the corresponding parameters of case class 'Music'\nval songDS = songDF.as[Music]\n\nsongDS.show(false)","user":"anonymous","dateUpdated":"2019-05-17T11:20:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","lineNumbers":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|artist       |loudness|\n+-------------+--------+\n|Frank Sinatra|-10.0   |\n|Beastie Boys |-5.0    |\n|Muse         |-7.0    |\n+-------------+--------+\n\nsongDS: org.apache.spark.sql.Dataset[Music] = [artist: string, loudness: double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=97","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=98"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558091959964_1078846113","id":"20190517-111919_1202397591","dateCreated":"2019-05-17T11:19:19+0000","dateStarted":"2019-05-17T11:20:18+0000","dateFinished":"2019-05-17T11:20:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28105"},{"text":"%md\n* Using a tuple","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Using a tuple</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613989_-875385580","id":"20190424-153614_1981224755","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28106"},{"text":"%spark\n// Conversion of the DataFrame 'songDF' into a Dataset by using the Dataset function '.as', which maps the columns of the DataFrame to a 2-sized tuple\nval songDsTuple = songDF.as[(String, Double)]\n\nsongDsTuple.show(false)","user":"anonymous","dateUpdated":"2019-05-17T11:20:31+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|artist       |loudness|\n+-------------+--------+\n|Frank Sinatra|-10.0   |\n|Beastie Boys |-5.0    |\n|Muse         |-7.0    |\n+-------------+--------+\n\nsongDsTuple: org.apache.spark.sql.Dataset[(String, Double)] = [artist: string, loudness: double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=99","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=100"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613989_2117885410","id":"20190424-153804_1177070445","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:20:31+0000","dateFinished":"2019-05-17T11:20:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28107"},{"text":"%md\n## 1.5.2 Converting a DataFrame to a RDD","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5.2 Converting a DataFrame to a RDD</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613989_-1048643939","id":"20190424-153814_1519235653","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28108"},{"text":"%md\n* To convert DataFrames to RDDs use `.rdd`","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>To convert DataFrames to RDDs use <code>.rdd</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613990_-788053671","id":"20190424-153822_1781635403","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28109"},{"text":"%md\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>","user":"anonymous","dateUpdated":"2019-05-17T11:11:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613990_2065380980","id":"20190424-153833_1882030658","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:11:29+0000","dateFinished":"2019-05-17T11:11:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28110"},{"text":"%spark\n// Conversion of the DataFrame 'songDF' into a RDD by using the Dataset function '.rdd'\nval songRDD = songDF.rdd\n\nsongRDD.collect.foreach(println)","user":"anonymous","dateUpdated":"2019-05-17T11:20:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Frank Sinatra,-10.0]\n[Beastie Boys,-5.0]\n[Muse,-7.0]\nsongRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[137] at rdd at <console>:49\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=101"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613990_-788839042","id":"20190424-153842_432886644","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:20:34+0000","dateFinished":"2019-05-17T11:20:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28111"},{"text":"%md\n<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' style='width:150px'>","user":"anonymous","dateUpdated":"2019-05-17T10:56:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' style='width:150px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613990_-196299235","id":"20190424-153857_291477756","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T10:56:42+0000","dateFinished":"2019-05-17T10:56:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28112"},{"text":"%pyspark\n# Read in data as a DataFrame\nsongDF = spark.read.load(dataPath)\n\n# Conversion of the DataFrame 'songDF' into a RDD by using the Dataset function '.rdd'\nsongRDD = songDF.rdd\n\nfor element in songRDD.collect():\n    print(element)","user":"anonymous","dateUpdated":"2019-05-17T11:20:38+0000","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(artist=u'Frank Sinatra', loudness=-10.0)\nRow(artist=u'Beastie Boys', loudness=-5.0)\nRow(artist=u'Muse', loudness=-7.0)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=102","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=103"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613991_1929606763","id":"20190424-153907_1187758323","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:20:39+0000","dateFinished":"2019-05-17T11:20:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28113"},{"text":"%md\n## 1.5.3 Converting a Dataset to a DataFrame","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5.3 Converting a Dataset to a DataFrame</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613991_-739129701","id":"20190424-153913_104827002","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28114"},{"text":"%md\n* To convert Datasets to DataFrames use `.toDF()`","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>To convert Datasets to DataFrames use <code>.toDF()</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613991_2058948286","id":"20190424-153922_1052000686","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28115"},{"text":"%md\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>","user":"anonymous","dateUpdated":"2019-05-17T11:12:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558091376811_2074006513","id":"20190517-110936_1846697671","dateCreated":"2019-05-17T11:09:36+0000","dateStarted":"2019-05-17T11:12:17+0000","dateFinished":"2019-05-17T11:12:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28116"},{"text":"%spark\n// Conversion of the Dataset 'songDS' into a DataFrame by using the Dataset function '.toDF()'\nval songDfDataset = songDS.toDF()\n\nsongDfDataset.show()","user":"anonymous","dateUpdated":"2019-05-17T11:20:43+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|       artist|loudness|\n+-------------+--------+\n|Frank Sinatra|   -10.0|\n| Beastie Boys|    -5.0|\n|         Muse|    -7.0|\n+-------------+--------+\n\nsongDfDataset: org.apache.spark.sql.DataFrame = [artist: string, loudness: double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=104","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=105"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613991_605593215","id":"20190424-153930_913227052","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:20:43+0000","dateFinished":"2019-05-17T11:20:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28117"},{"text":"%md\n## 1.5.4 Converting a Dataset to a RDD","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5.4 Converting a Dataset to a RDD</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613991_1998402491","id":"20190424-153939_418218620","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28118"},{"text":"%md\n* To convert Datasets to RDDs use `.rdd`","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>To convert Datasets to RDDs use <code>.rdd</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613992_-1511587462","id":"20190424-153946_843897463","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28119"},{"text":"%md\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>","user":"anonymous","dateUpdated":"2019-05-17T11:12:22+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558091540104_-1135759974","id":"20190517-111220_2096414825","dateCreated":"2019-05-17T11:12:20+0000","dateStarted":"2019-05-17T11:12:22+0000","dateFinished":"2019-05-17T11:12:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28120"},{"text":"%spark\n// Conversion of the Dataset 'songDS' of type case class 'Music' into a RDD by using the Dataset function '.rdd'\nval songRddDatasetCaseClass = songDS.rdd\n\n// Returns RDD of type case class 'Music'\nsongRddDatasetCaseClass.collect.foreach(println)\n\n// Conversion of the Dataset 'songDsTuple' of type tuple (2-sized) into a RDD by using the Dataset function '.rdd'\nval songRddDatasetTuple = songDsTuple.rdd\n\n// Returns RDD of type tuple (2-sized)\nsongRddDatasetTuple.collect.foreach(println)","user":"anonymous","dateUpdated":"2019-05-17T11:33:10+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Music(Frank Sinatra,-10.0)\nMusic(Beastie Boys,-5.0)\nMusic(Muse,-7.0)\n(Frank Sinatra,-10.0)\n(Beastie Boys,-5.0)\n(Muse,-7.0)\nsongRddDatasetCaseClass: org.apache.spark.rdd.RDD[Music] = MapPartitionsRDD[151] at rdd at <console>:53\nsongRddDatasetTuple: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[155] at rdd at <console>:57\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=114","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=115"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613992_-1277488808","id":"20190424-153955_966392820","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:24:18+0000","dateFinished":"2019-05-17T11:24:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28121"},{"text":"%md\n## 1.5.5 Converting a RDD to a DataFrame","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5.5 Converting a RDD to a DataFrame</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613992_482907474","id":"20190424-154010_1405105532","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28122"},{"text":"%md\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>","user":"anonymous","dateUpdated":"2019-05-17T11:24:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://hazelcast.org/wp-content/uploads/2016/04/scala-logo.jpg' style='width:100px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613993_-967350513","id":"20190424-161531_1366239576","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:24:52+0000","dateFinished":"2019-05-17T11:24:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28124"},{"text":"%md\n* If the RDD is strongly typed (e.g. through case class or tuple) you can use `.toDF()` again.","user":"anonymous","dateUpdated":"2019-05-17T15:58:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558108530422_-252580464","id":"20190517-155530_512209544","dateCreated":"2019-05-17T15:55:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33544","dateFinished":"2019-05-17T15:58:43+0000","dateStarted":"2019-05-17T15:58:43+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>If the RDD is strongly typed (e.g. through case class or tuple) you can use <code>.toDF()</code> again.</li>\n</ul>\n</div>"}]}},{"text":"%spark\n// Strongly typed due to case class\nval songDfCaseClass = songRddDatasetCaseClass.toDF()  \n\nsongDfCaseClass.show()\n\n// Strongly typed due to tuple\nval songDfTuple = songRddDatasetTuple.toDF()\n\nsongDfTuple.show()\n\n// Also strongly typed\nval songRddInt = sc.parallelize(Seq(1,2,3))   // RDD[Int]\nval songRddIntDF = songRddInt.toDF()\n\nsongRddIntDF.show()","user":"anonymous","dateUpdated":"2019-05-17T14:10:55+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":542,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|       artist|loudness|\n+-------------+--------+\n|Frank Sinatra|   -10.0|\n| Beastie Boys|    -5.0|\n|         Muse|    -7.0|\n+-------------+--------+\n\n+-------------+-----+\n|           _1|   _2|\n+-------------+-----+\n|Frank Sinatra|-10.0|\n| Beastie Boys| -5.0|\n|         Muse| -7.0|\n+-------------+-----+\n\n+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n+-----+\n\nsongDfCaseClass: org.apache.spark.sql.DataFrame = [artist: string, loudness: double]\nsongDfTuple: org.apache.spark.sql.DataFrame = [_1: string, _2: double]\nsongRddInt: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[452] at parallelize at <console>:85\nsongRddIntDF: org.apache.spark.sql.DataFrame = [value: int]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=231","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=232","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=233","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=234","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=235","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=236"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613993_-2095475234","id":"20190424-161544_1165201400","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T14:06:51+0000","dateFinished":"2019-05-17T14:06:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28125"},{"text":"%md\n* If this isn´t the case (i.e. `RDD[Row]`) you have to map each element into a RDD of type case class respectively of type tuple and convert it afterwards\nwith `.toDF()` to a DataFrame.","user":"anonymous","dateUpdated":"2019-05-17T15:55:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>If this isn´t the case (i.e. <code>RDD[Row]</code>) you have to map each element into a RDD of type case class respectively of type tuple and convert it afterwards<br/>with <code>.toDF()</code> to a DataFrame.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613993_1113260120","id":"20190424-161549_1822860009","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T15:55:53+0000","dateFinished":"2019-05-17T15:55:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28126"},{"text":"%spark\n// Check type of RDD 'songRDD'\nval check = songRDD.map(x => x)   // .map(x => x) is just a pseudo transformation which is not manipulating the data at all. Instead it´s possible to check the type of the RDD 'songRDD' in the output ot this paragraph (=> RDD[Row])\n\n// Create case class\ncase class Music(artist: String, loudness: Double)\n\n// Map elements to case class 'Music' and convert the resulting RDD[Music] to a DataFrame by calling '.toDF()'\nval songDfRDD = songRDD.map(row => Music(row.getAs[String](0), row.getAs[Double](1))).toDF()\n\nsongDfRDD.show()\n\n// Map elements to a 2-sized tuple and convert the resulting RDD[(String, Double)] to a DataFrame by calling '.toDF()'\nval songDfTuple = songRDD.map(row => (row.getAs[String](0), row.getAs[Double](1))).toDF()\n\nsongDfTuple.show()","user":"anonymous","dateUpdated":"2019-05-17T15:01:11+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":424,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|artist       |loudness|\n+-------------+--------+\n|Frank Sinatra|-10.0   |\n|Beastie Boys |-5.0    |\n|Muse         |-7.0    |\n+-------------+--------+\n\n+-------------+-----+\n|_1           |_2   |\n+-------------+-----+\n|Frank Sinatra|-10.0|\n|Beastie Boys |-5.0 |\n|Muse         |-7.0 |\n+-------------+-----+\n\nimport sqlContext.implicits._\ncheck: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[518] at map at <console>:22\ndefined class Music\nsongDfRDD: org.apache.spark.sql.DataFrame = [artist: string, loudness: double]\nsongDfTuple: org.apache.spark.sql.DataFrame = [_1: string, _2: double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=264","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=265","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=266","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=267"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558088613993_807253005","id":"20190424-161640_1979102466","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T14:14:20+0000","dateFinished":"2019-05-17T14:14:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28127"},{"text":"%md\n<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' style='width:150px'>","user":"anonymous","dateUpdated":"2019-05-17T11:34:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/Python_logo_and_wordmark.svg' style='width:150px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613994_-186768286","id":"20190424-161653_208671266","dateCreated":"2019-05-17T10:23:33+0000","dateStarted":"2019-05-17T11:34:09+0000","dateFinished":"2019-05-17T11:34:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28128"},{"text":"%md\n* With Python there are only two ways to convert a RDD into a DataFrame. Both use `.toDF()`. The first method requires you to bring every element of a RDD into a tupel","user":"anonymous","dateUpdated":"2019-05-17T15:56:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558107479771_-500138878","id":"20190517-153759_942569735","dateCreated":"2019-05-17T15:37:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:32691","dateFinished":"2019-05-17T15:56:27+0000","dateStarted":"2019-05-17T15:56:27+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>With Python there are only two ways to convert a RDD into a DataFrame. Both use <code>.toDF()</code>. The first method requires you to bring every element of a RDD into a tupel</li>\n</ul>\n</div>"}]}},{"text":"%pyspark\n# The first method requires to bring every element of a RDD into a tupel\n# Inside of '.toDF' we define the column names\nsongRddTupel = sc.parallelize([(1,2,3),(4,5,6)]).toDF([\"one\", \"two\", \"three\"])\n\nsongRddTupel.show()","user":"anonymous","dateUpdated":"2019-05-17T15:48:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---+-----+\n|one|two|three|\n+---+---+-----+\n|  1|  2|    3|\n|  4|  5|    6|\n+---+---+-----+\n\n+---+---+-----+\n|one|two|three|\n+---+---+-----+\n|  1|  2|    3|\n|  4|  5|    6|\n|  7|  8|    9|\n+---+---+-----+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=488","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=489","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=490","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=491","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=492","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=493","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=494","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=495"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1558094479297_1148565213","id":"20190517-120119_665989263","dateCreated":"2019-05-17T12:01:19+0000","dateStarted":"2019-05-17T15:45:02+0000","dateFinished":"2019-05-17T15:45:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28129"},{"text":"%md\n* The second method requires you to bring every element of a RDD into a row object.","user":"anonymous","dateUpdated":"2019-05-17T15:56:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558108091393_-692004991","id":"20190517-154811_2136885282","dateCreated":"2019-05-17T15:48:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33134","dateFinished":"2019-05-17T15:56:35+0000","dateStarted":"2019-05-17T15:56:35+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>The second method requires you to bring every element of a RDD into a row object.</li>\n</ul>\n</div>"}]}},{"text":"%pyspark\nfrom pyspark.sql import Row\n\n# The second method requires to bring every element of a RDD into a Row object\n# The columns are specified inside the row objects\nsongDfRow = sc.parallelize([Row(one=1,two=2,three=3),Row(one=4,two=5,three=6),Row(one=7,two=8,three=9)]).toDF()\nsongDfRow.show()\n\n# Alternative second method\n# The following lines reflect also the second method but this time the columns are specified inside '.toDF()'\nsongDfRow2 = sc.parallelize([Row(1,2,3),Row(4,5,6),Row(7,8,9)]).toDF([\"one\", \"two\", \"three\"])\nsongDfRow2.show()","user":"anonymous","dateUpdated":"2019-05-17T15:54:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1558108088119_2080752983","id":"20190517-154808_105040240","dateCreated":"2019-05-17T15:48:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33062","dateFinished":"2019-05-17T15:54:50+0000","dateStarted":"2019-05-17T15:54:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+---+\n|one|three|two|\n+---+-----+---+\n|  1|    3|  2|\n|  4|    6|  5|\n|  7|    9|  8|\n+---+-----+---+\n\n+---+---+-----+\n|one|two|three|\n+---+---+-----+\n|  1|  2|    3|\n|  4|  5|    6|\n|  7|  8|    9|\n+---+---+-----+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=526","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=527","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=528","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=529","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=530","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=531","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=532"],"interpreterSettingId":"spark"}}},{"text":"%md\n## 1.5.6 Converting a RDD to a Dataset","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5.6 Converting a RDD to a Dataset</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613994_872274472","id":"20190424-161718_992015717","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28131"},{"text":"%md\n* If the RDD is strongly typed (e.g. through case class or tuple) you can use `.toDS()`.","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>If the RDD is strongly typed (e.g. through case class or tuple) you can use <code>.toDS()</code>.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613994_-976375234","id":"20190424-161724_2070368091","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28132"},{"text":"%spark\n// Strongly typed due to case class\nval songDsCaseClass = songRddDatasetCaseClass.toDS()\nsongDsCaseClass.show()\n\n// Strongly typed due to tuple\nval songDsTuple = songRddDatasetTuple.toDS()\nsongDsTuple.show()\n\n// Also strongly typed\nval songRddInt = sc.parallelize(Seq(1,2,3))   // RDD[Int]\nval songRddIntDF = songRddInt.toDS()\n\nsongRddIntDF.show()","user":"anonymous","dateUpdated":"2019-05-17T16:00:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":492,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|       artist|loudness|\n+-------------+--------+\n|Frank Sinatra|   -10.0|\n| Beastie Boys|    -5.0|\n|         Muse|    -7.0|\n+-------------+--------+\n\n+-------------+-----+\n|           _1|   _2|\n+-------------+-----+\n|Frank Sinatra|-10.0|\n| Beastie Boys| -5.0|\n|         Muse| -7.0|\n+-------------+-----+\n\n+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n+-----+\n\nsongDsCaseClass: org.apache.spark.sql.Dataset[Music] = [artist: string, loudness: double]\nsongDsTuple: org.apache.spark.sql.Dataset[(String, Double)] = [_1: string, _2: double]\nsongRddInt: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1294] at parallelize at <console>:108\nsongRddIntDF: org.apache.spark.sql.Dataset[Int] = [value: int]\n"}]},"apps":[],"jobName":"paragraph_1558088613995_511609074","id":"20190424-161734_1638365034","dateCreated":"2019-05-17T10:23:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28133","dateFinished":"2019-05-17T15:59:58+0000","dateStarted":"2019-05-17T15:59:57+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=545","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=546","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=547","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=548","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=549","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=550"],"interpreterSettingId":"spark"}}},{"text":"%md\n* If this isn´t the case (i.e. RDD of type row -> RDD[Row]) you have to map each element into a RDD of type case class respectively of type tuple and convert it afterwards\nwith `.toDS()` to a Dataset.","user":"anonymous","dateUpdated":"2019-05-17T10:23:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>If this isn´t the case (i.e. RDD of type row -&gt; RDD[Row]) you have to map each element into a RDD of type case class respectively of type tuple and convert it afterwards<br/>with <code>.toDS()</code> to a Dataset.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1558088613995_1420461916","id":"20190424-161746_1841330015","dateCreated":"2019-05-17T10:23:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28134"},{"text":"%spark\n// Check type of RDD 'songRDD'\nval check = songRDD.map(x => x)   // .map(x => x) is just a pseudo transformation which is not manipulating the data at all. Instead it´s possible to check the type of the RDD 'songRDD' in the output ot this paragraph (=> RDD[Row])\n\n// Create case class\ncase class Music(artist: String, loudness: Double)\n\n//  Map elements to case class 'Music' and convert the resulting RDD[Music] to a Dataset by calling '.toDS()'\nval songDsRDD = songRDD.map(row => Music(row.getAs[String](0), row.getAs[Double](1))).toDS()\n\nsongDsRDD.show()\n\n// Map elements to a 2-sized tuple and convert the resulting RDD[(String, Double)] to a Dataset by calling '.toDS()'\nval songDsTuple = songRDD.map(row => (row.getAs[String](0), row.getAs[Double](1))).toDS()\n\nsongDsTuple.show()","user":"anonymous","dateUpdated":"2019-05-17T16:02:28+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":444,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+--------+\n|       artist|loudness|\n+-------------+--------+\n|Frank Sinatra|   -10.0|\n| Beastie Boys|    -5.0|\n|         Muse|    -7.0|\n+-------------+--------+\n\n+-------------+-----+\n|           _1|   _2|\n+-------------+-----+\n|Frank Sinatra|-10.0|\n| Beastie Boys| -5.0|\n|         Muse| -7.0|\n+-------------+-----+\n\nimport sqlContext.implicits._\ncheck: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[1267] at map at <console>:21\ndefined class Music\nsongDsRDD: org.apache.spark.sql.Dataset[Music] = [artist: string, loudness: double]\nsongDsTuple: org.apache.spark.sql.Dataset[(String, Double)] = [_1: string, _2: double]\n"}]},"apps":[],"jobName":"paragraph_1558088613995_1388618065","id":"20190424-161753_1752176734","dateCreated":"2019-05-17T10:23:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28135","dateFinished":"2019-05-17T15:58:07+0000","dateStarted":"2019-05-17T15:58:00+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=537","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=538","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=539","http://ip-172-31-42-97.eu-central-1.compute.internal:4040/jobs/job?id=540"],"interpreterSettingId":"spark"}}}],"name":"/1. DataFrames, Datasets & RDDs/1.5 Conversion between DataFrames, Datasets and RDDs","id":"2ECQQJP7S","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}