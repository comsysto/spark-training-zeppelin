{"paragraphs":[{"text":"%md <img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532380_-434411260","id":"20190418-112806_1412543353","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18847"},{"text":"%md\n# Apache Spark Training ","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Apache Spark Training</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532381_-1393637439","id":"20190418-112826_1133266129","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18848"},{"text":"%md\n## Outline\n\n**1.** [**DataFrames, Datasets & RDDs**](http://localhost:8080/#/notebook/2EBM9JWJR)\n\n**2.** [**Operations: Transformations and Actions**](http://localhost:8080/#/notebook/2E8E18R98)\n\n**3.** [**Ingestion & Saving**](http://localhost:8080/#/notebook/2E8NZTFZG)\n\n**4.** [**Spark SQL**](http://localhost:8080/#/notebook/2EASBV4HN)\n\n**5.** [**Performance Tuning**](http://localhost:8080/#/notebook/2E9X8V23M)","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Outline</h2>\n<p><strong>1.</strong> <a href=\"http://localhost:8080/#/notebook/2EBM9JWJR\"><strong>DataFrames, Datasets &amp; RDDs</strong></a></p>\n<p><strong>2.</strong> <a href=\"http://localhost:8080/#/notebook/2E8E18R98\"><strong>Operations: Transformations and Actions</strong></a></p>\n<p><strong>3.</strong> <a href=\"http://localhost:8080/#/notebook/2E8NZTFZG\"><strong>Ingestion &amp; Saving</strong></a></p>\n<p><strong>4.</strong> <a href=\"http://localhost:8080/#/notebook/2EASBV4HN\"><strong>Spark SQL</strong></a></p>\n<p><strong>5.</strong> <a href=\"http://localhost:8080/#/notebook/2E9X8V23M\"><strong>Performance Tuning</strong></a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532381_655214728","id":"20190424-123348_1022128842","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18849"},{"text":"%md ## Get workbooks with Spark presentation incl. exercises\n- Login with your own account\n- Clone the notebook with the exercise into your users workspace\n- Attach the notebook to the cluster with your name\n- Try to solve it by your own without looking at the solution\n- Scala or Python knowledge necessary\n  - [Scala Cheat Sheet](http://localhost:8080/#/notebook/2E9H24PSY)\n  - [Python Cheat Sheet](http://localhost:8080/#/notebook/2E8RC5W3Z)","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Get workbooks with Spark presentation incl. exercises</h2>\n<ul>\n  <li>Login with your own account</li>\n  <li>Clone the notebook with the exercise into your users workspace</li>\n  <li>Attach the notebook to the cluster with your name</li>\n  <li>Try to solve it by your own without looking at the solution</li>\n  <li>Scala or Python knowledge necessary</li>\n  <li><a href=\"http://localhost:8080/#/notebook/2E9H24PSY\">Scala Cheat Sheet</a></li>\n  <li><a href=\"http://localhost:8080/#/notebook/2E8RC5W3Z\">Python Cheat Sheet</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532382_-1736870799","id":"20190424-123411_246899722","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18850"},{"text":"%md\n## General Spark Introduction\n### Apache Hadoop\n<img src=\"https://www.cloudera.com/content/dam/www/marketing/images/diagrams/xcdh-diagram1.png.pagespeed.ic.c3N6-xkI0O.webp\" style=\"width:600px\">","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>General Spark Introduction</h2>\n<h3>Apache Hadoop</h3>\n<img src=\"https://www.cloudera.com/content/dam/www/marketing/images/diagrams/xcdh-diagram1.png.pagespeed.ic.c3N6-xkI0O.webp\" style=\"width:600px\">\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532382_-1831546249","id":"20190424-123539_1284738474","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18851"},{"text":"%md\n## Distributed Processing - Apache YARN\n<img src=\"http://spark.apache.org/docs/latest/img/cluster-overview.png\" style=\"width:800px\">","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Distributed Processing - Apache YARN</h2>\n<img src=\"http://spark.apache.org/docs/latest/img/cluster-overview.png\" style=\"width:800px\">\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532383_667011177","id":"20190424-123601_43305341","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18852"},{"text":"%md\n## Apache Spark\n\n<img src='https://spark.apache.org/images/spark-logo-trademark.png'>  \n\n### Why is Apache Spark so interesting?\n\n*Current Spark Version*\n- 2.4\n\n*General parallel data processing engine*\n- Fast (in-memory)\n- Easy to use\n- Scalable (on a single instance or on a cluster)\n\n*Functional programming style*\n\n*Built in support for different data sources and formats*\n- Local filesystem, HDFS, AWS S3, ...\n- JSON, Parquet, CSV, ...\n\n*Supports standard SQL queries on different data formats*\n\n*Variety of powerful libraries*\n- Core Spark\n- Spark SQL\n- MLlib\n- Spark Streaming\n- GraphX\n- Support for Deep Learning libraries (e. g. TensorFlow)\n\n*Usage in different environments*\n\n- Simple use it as a library inside your Java application\n- With R and RStudio\n- With Python and Jupyter Notebook\n\n*Vibrant developer community*\n\n### Downsides\n\n*Cluster computation is complex*\n- Data locality\n- Monitoring\n- Communication overhead\n- Maintaining cluster infrastructure\n\n*Careful adjustment of available memory and configuration needed*\n\n*A lot of dependencies/other libraries which may conflict with your project*","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Apache Spark</h2>\n<img src='https://spark.apache.org/images/spark-logo-trademark.png'>\n<h3>Why is Apache Spark so interesting?</h3>\n<p><em>Current Spark Version</em><br/>- 2.4</p>\n<p><em>General parallel data processing engine</em><br/>- Fast (in-memory)<br/>- Easy to use<br/>- Scalable (on a single instance or on a cluster)</p>\n<p><em>Functional programming style</em></p>\n<p><em>Built in support for different data sources and formats</em><br/>- Local filesystem, HDFS, AWS S3, &hellip;<br/>- JSON, Parquet, CSV, &hellip;</p>\n<p><em>Supports standard SQL queries on different data formats</em></p>\n<p><em>Variety of powerful libraries</em><br/>- Core Spark<br/>- Spark SQL<br/>- MLlib<br/>- Spark Streaming<br/>- GraphX<br/>- Support for Deep Learning libraries (e. g. TensorFlow)</p>\n<p><em>Usage in different environments</em></p>\n<ul>\n  <li>Simple use it as a library inside your Java application</li>\n  <li>With R and RStudio</li>\n  <li>With Python and Jupyter Notebook</li>\n</ul>\n<p><em>Vibrant developer community</em></p>\n<h3>Downsides</h3>\n<p><em>Cluster computation is complex</em><br/>- Data locality<br/>- Monitoring<br/>- Communication overhead<br/>- Maintaining cluster infrastructure</p>\n<p><em>Careful adjustment of available memory and configuration needed</em></p>\n<p><em>A lot of dependencies/other libraries which may conflict with your project</em></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532384_-999572538","id":"20190424-123649_90887915","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18853"},{"text":"%md\n## SparkSession and SparkContext\n\n* Spark programs in general always need one [`SparkContext`](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.SparkContext) object.\n* In addition you need at least one [`SparkSession`](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession) object if you want to use Spark SQL.\n\n### [`SparkContext`](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.SparkContext)\n\n* Main entry point for core Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n* Contained in the driver program.\n* Programmatically requests resources from the Worker Nodes via the Cluster Manager.\n* Communicates with the cluster to distribute \"work\" or tasks.\n* The Spark shell and notebooks provide a preconfigured SparkContext object called `sc`.\n* Creation will be explained later when we discuss [`SparkSession`](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession).\n* In standalone applications `sc.stop()` needs to be called for terminating the SparkContext if no SparkSession exists. \n\n### [`SparkSession`](http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession)\n\n* The entry point to programming Spark with the Dataset and DataFrame API.\n* The Spark shell and notebooks provide a preconfigured SparkSession object called `spark`.\n* In standalone application you have to build your own SparkSession with the SparkSession builder.\n* The SparkSession builder automatically provides a SparkContext object too.\n* In standalone applications `spark.stop()` needs to be called for terminating the SparkSession. ","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>SparkSession and SparkContext</h2>\n<ul>\n  <li>Spark programs in general always need one <a href=\"http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.SparkContext\"><code>SparkContext</code></a> object.</li>\n  <li>In addition you need at least one <a href=\"http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession\"><code>SparkSession</code></a> object if you want to use Spark SQL.</li>\n</ul>\n<h3><a href=\"http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.SparkContext\"><code>SparkContext</code></a></h3>\n<ul>\n  <li>Main entry point for core Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.</li>\n  <li>Contained in the driver program.</li>\n  <li>Programmatically requests resources from the Worker Nodes via the Cluster Manager.</li>\n  <li>Communicates with the cluster to distribute &ldquo;work&rdquo; or tasks.</li>\n  <li>The Spark shell and notebooks provide a preconfigured SparkContext object called <code>sc</code>.</li>\n  <li>Creation will be explained later when we discuss <a href=\"http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession\"><code>SparkSession</code></a>.</li>\n  <li>In standalone applications <code>sc.stop()</code> needs to be called for terminating the SparkContext if no SparkSession exists.</li>\n</ul>\n<h3><a href=\"http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.SparkSession\"><code>SparkSession</code></a></h3>\n<ul>\n  <li>The entry point to programming Spark with the Dataset and DataFrame API.</li>\n  <li>The Spark shell and notebooks provide a preconfigured SparkSession object called <code>spark</code>.</li>\n  <li>In standalone application you have to build your own SparkSession with the SparkSession builder.</li>\n  <li>The SparkSession builder automatically provides a SparkContext object too.</li>\n  <li>In standalone applications <code>spark.stop()</code> needs to be called for terminating the SparkSession.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532384_2138874745","id":"20190424-123723_1482232079","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18854"},{"text":"%md\n## Deployment modes\n\n*   *Standalone*:\n    *   Uses a simple cluster manager included with Spark.\n    *   Makes it easy to set-up a cluster.\n    *   Ideal for local development.\n*   *Hadoop YARN*:\n    *   Cluster Manager used in Hadoop 2.\n    *   Used on Amazon EMR.\n*   Further possibilities:\n    *   *Apache Mesos*\n    *   *Kubernetes*","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Deployment modes</h2>\n<ul>\n  <li><em>Standalone</em>:\n    <ul>\n      <li>Uses a simple cluster manager included with Spark.</li>\n      <li>Makes it easy to set-up a cluster.</li>\n      <li>Ideal for local development.</li>\n    </ul>\n  </li>\n  <li><em>Hadoop YARN</em>:\n    <ul>\n      <li>Cluster Manager used in Hadoop 2.</li>\n      <li>Used on Amazon EMR.</li>\n    </ul>\n  </li>\n  <li>Further possibilities:\n    <ul>\n      <li><em>Apache Mesos</em></li>\n      <li><em>Kubernetes</em></li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532384_-1099299598","id":"20190424-123732_522647549","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18855"},{"text":"%md\n## API Docs\n\n### [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package)\n\n### [Python](http://spark.apache.org/docs/latest/api/python/index.html)\n\n### [SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>API Docs</h2>\n<h3><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\">Scala</a></h3>\n<h3><a href=\"http://spark.apache.org/docs/latest/api/python/index.html\">Python</a></h3>\n<h3><a href=\"https://spark.apache.org/docs/latest/api/sql/index.html\">SQL, Built-in Functions</a></h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532385_-981387406","id":"20190424-123743_213678309","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18856"},{"text":"%md\n## Projects at comSysto using Spark\n\n### Automotive:\n- Analyze raw sensor data of a car fleet and extract/aggregate geographical information\n- Infrastructure: AWS EMR (on demand)\n- Stream and Batch Processing\n- Clients: Zeppelin notebooks, Jupyter, R Studio\n- Data store: Parquet\n\n### Home appliances:\n- Analyze sensor data of home appliances for program detection and reporting\n- Infrastructure: AWS S3 and AWS EMR (Hadoop, Spark/Scala) cluster connected to SAP HANA\n- Cluster for daily batches and Cluster for Data Science tasks\n- Clients: Zeppelin notebooks, R Studio\n- Data formats: JSON, Parquet, CSV\n\n### Other companies are continuously requesting Spark support","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Projects at comSysto using Spark</h2>\n<h3>Automotive:</h3>\n<ul>\n  <li>Analyze raw sensor data of a car fleet and extract/aggregate geographical information</li>\n  <li>Infrastructure: AWS EMR (on demand)</li>\n  <li>Stream and Batch Processing</li>\n  <li>Clients: Zeppelin notebooks, Jupyter, R Studio</li>\n  <li>Data store: Parquet</li>\n</ul>\n<h3>Home appliances:</h3>\n<ul>\n  <li>Analyze sensor data of home appliances for program detection and reporting</li>\n  <li>Infrastructure: AWS S3 and AWS EMR (Hadoop, Spark/Scala) cluster connected to SAP HANA</li>\n  <li>Cluster for daily batches and Cluster for Data Science tasks</li>\n  <li>Clients: Zeppelin notebooks, R Studio</li>\n  <li>Data formats: JSON, Parquet, CSV</li>\n</ul>\n<h3>Other companies are continuously requesting Spark support</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532385_-985603835","id":"20190424-123754_373014856","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18857"},{"text":"%md\n## Spark Glossary\n### [**Spark Glossary**](https://spark.apache.org/docs/latest/cluster-overview.html#glossary)\n\nTerm     | Meaning\n  ------------- | -------------\n  Application  | User program built on Spark. Consists of a driver program and executors on the cluster.\n  Application jar                     | A jar containing the user's Spark application. In some cases users will want to create an \"uber jar\" containing their application along with its dependencies. The user's jar should never include Hadoop or Spark libraries, however, these will be added at runtime.\n  Driver program  | The process running the main() function of the application and creating the SparkContext.\n  Cluster manager  | An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN).\n  Deploy mode  | Distinguishes where the driver process runs. In \"cluster\" mode, the framework launches the driver inside of the cluster. In \"client\" mode, the submitter launches the driver outside of the cluster.\n  Worker node  | Any node that can run application code in the cluster.\n  Executor  | A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.\n  Task  | A unit of work that will be sent to one executor.\n  Job  | A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you'll see this term used in the driver's logs.\n  Stage  | Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs.\n  <img width=200/>|<img width=500/>","user":"anonymous","dateUpdated":"2019-05-15T09:12:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Spark Glossary</h2>\n<h3><a href=\"https://spark.apache.org/docs/latest/cluster-overview.html#glossary\"><strong>Spark Glossary</strong></a></h3>\n<table>\n  <thead>\n    <tr>\n      <th>Term </th>\n      <th>Meaning</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Application </td>\n      <td>User program built on Spark. Consists of a driver program and executors on the cluster.</td>\n    </tr>\n    <tr>\n      <td>Application jar </td>\n      <td>A jar containing the user&rsquo;s Spark application. In some cases users will want to create an &ldquo;uber jar&rdquo; containing their application along with its dependencies. The user&rsquo;s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</td>\n    </tr>\n    <tr>\n      <td>Driver program </td>\n      <td>The process running the main() function of the application and creating the SparkContext.</td>\n    </tr>\n    <tr>\n      <td>Cluster manager </td>\n      <td>An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN).</td>\n    </tr>\n    <tr>\n      <td>Deploy mode </td>\n      <td>Distinguishes where the driver process runs. In &ldquo;cluster&rdquo; mode, the framework launches the driver inside of the cluster. In &ldquo;client&rdquo; mode, the submitter launches the driver outside of the cluster.</td>\n    </tr>\n    <tr>\n      <td>Worker node </td>\n      <td>Any node that can run application code in the cluster.</td>\n    </tr>\n    <tr>\n      <td>Executor </td>\n      <td>A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</td>\n    </tr>\n    <tr>\n      <td>Task </td>\n      <td>A unit of work that will be sent to one executor.</td>\n    </tr>\n    <tr>\n      <td>Job </td>\n      <td>A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you&rsquo;ll see this term used in the driver&rsquo;s logs.</td>\n    </tr>\n    <tr>\n      <td>Stage </td>\n      <td>Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you&rsquo;ll see this term used in the driver&rsquo;s logs.</td>\n    </tr>\n    <tr>\n      <td><img width=200/></td>\n      <td><img width=500/></td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"jobName":"paragraph_1557911532386_1314265852","id":"20190426-141837_1392892610","dateCreated":"2019-05-15T09:12:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:18858"}],"name":"/0. Overview/0. Overview","id":"2ECQPWY6A","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}