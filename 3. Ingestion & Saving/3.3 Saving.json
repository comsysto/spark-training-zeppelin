{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121206711_-1426163219","id":"20190424-175326_1524654233","dateCreated":"2019-04-24T17:53:26+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54131","text":"%md <img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>","dateUpdated":"2019-04-24T17:53:36+0200","dateFinished":"2019-04-24T17:53:36+0200","dateStarted":"2019-04-24T17:53:36+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>\n</div>"}]}},{"text":"%md\n# 3.3 Saving","user":"anonymous","dateUpdated":"2019-04-24T17:53:47+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121216839_-189476104","id":"20190424-175336_451239144","dateCreated":"2019-04-24T17:53:36+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54209","dateFinished":"2019-04-24T17:53:47+0200","dateStarted":"2019-04-24T17:53:47+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3.3 Saving</h1>\n</div>"}]}},{"text":"%md\n***3.3.1 Saving DataFrames and Datasets***\n\n* Common `.DataFrameWriter` settings\n\n* Example for saving DataFrames and Datasets\n\n***3.3.2 Saving RDDs***\n\n* Saving RDDs as text files with `.saveAsTextFile` or with `.saveAsHadoopFile`for other file formats\n\n* Example for RDD ingestion out of a single text file","user":"anonymous","dateUpdated":"2019-04-24T17:53:56+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121227382_-482313305","id":"20190424-175347_1997058411","dateCreated":"2019-04-24T17:53:47+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54303","dateFinished":"2019-04-24T17:53:56+0200","dateStarted":"2019-04-24T17:53:56+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong><em>3.3.1 Saving DataFrames and Datasets</em></strong></p>\n<ul>\n  <li>\n  <p>Common <code>.DataFrameWriter</code> settings</p></li>\n  <li>\n  <p>Example for saving DataFrames and Datasets</p></li>\n</ul>\n<p><strong><em>3.3.2 Saving RDDs</em></strong></p>\n<ul>\n  <li>\n  <p>Saving RDDs as text files with <code>.saveAsTextFile</code> or with <code>.saveAsHadoopFile</code>for other file formats</p></li>\n  <li>\n  <p>Example for RDD ingestion out of a single text file</p></li>\n</ul>\n</div>"}]}},{"text":"%md\n## 3.3.1 Saving Dataframes and Datasets\n\nA DataFrame can be saved with `.write` which creates a `DataFrameWriter` object.\nWith the settings of the `DataFrameWriter` it´s possbile to influence the process of saving data.\n\nDatasets get saved as DataFrames.","user":"anonymous","dateUpdated":"2019-04-24T17:54:08+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121236055_902642643","id":"20190424-175356_159849096","dateCreated":"2019-04-24T17:53:56+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54396","dateFinished":"2019-04-24T17:54:08+0200","dateStarted":"2019-04-24T17:54:08+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.3.1 Saving Dataframes and Datasets</h2>\n<p>A DataFrame can be saved with <code>.write</code> which creates a <code>DataFrameWriter</code> object.<br/>With the settings of the <code>DataFrameWriter</code> it´s possbile to influence the process of saving data.</p>\n<p>Datasets get saved as DataFrames.</p>\n</div>"}]}},{"text":"%md\n### Common DataFrameWriter settings\n\n*   `.format`: Specifies the type of the data source (e.g. CSV, JSON, Parquet, etc.)\n\n*   `.mode`: Specifies the handling if directory and/or file respectively table already exists (e.g. error, overwrite, append, ignore)\n\n*   `.partitionBy`: Specifies the partitioning of the data\n\n*   `.option`: Additional key-value settings (e.g. \"Header\",\"true\")\n\n*   `.save`:   For saving data to files. `.save` without `.format` reads in Parquet as default.\n\n*   `.saveAsTable`:  For saving data out as Hive tables\n\n*   `.csv`, `.json`, etc.: Instead of using `.save` one can use format specific short cuts.","user":"anonymous","dateUpdated":"2019-04-24T17:54:16+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121243014_-2078794802","id":"20190424-175403_1279755413","dateCreated":"2019-04-24T17:54:03+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54486","dateFinished":"2019-04-24T17:54:16+0200","dateStarted":"2019-04-24T17:54:16+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Common DataFrameWriter settings</h3>\n<ul>\n  <li>\n  <p><code>.format</code>: Specifies the type of the data source (e.g. CSV, JSON, Parquet, etc.)</p></li>\n  <li>\n  <p><code>.mode</code>: Specifies the handling if directory and/or file respectively table already exists (e.g. error, overwrite, append, ignore)</p></li>\n  <li>\n  <p><code>.partitionBy</code>: Specifies the partitioning of the data</p></li>\n  <li>\n  <p><code>.option</code>: Additional key-value settings (e.g. &ldquo;Header&rdquo;,&ldquo;true&rdquo;)</p></li>\n  <li>\n  <p><code>.save</code>: For saving data to files. <code>.save</code> without <code>.format</code> reads in Parquet as default.</p></li>\n  <li>\n  <p><code>.saveAsTable</code>: For saving data out as Hive tables</p></li>\n  <li>\n  <p><code>.csv</code>, <code>.json</code>, etc.: Instead of using <code>.save</code> one can use format specific short cuts.</p></li>\n</ul>\n</div>"}]}},{"text":"%md\n### Example for DataFrame savings","user":"anonymous","dateUpdated":"2019-04-24T17:54:24+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121256821_1128001412","id":"20190424-175416_632569445","dateCreated":"2019-04-24T17:54:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54599","dateFinished":"2019-04-24T17:54:24+0200","dateStarted":"2019-04-24T17:54:24+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for DataFrame savings</h3>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:54:32+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121264102_1305007623","id":"20190424-175424_1969171016","dateCreated":"2019-04-24T17:54:24+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54689","dateFinished":"2019-04-24T17:54:32+0200","dateStarted":"2019-04-24T17:54:32+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"import org.apache.spark.sql.functions._\n\n//Read in Parquet file\nval songDF = spark.read.load(\"dbfs:/cs-spark-training/Songs/\")\nsongDF.show(false)\n\n//Operate on 'songDF'\nval songDfChanged = songDF.withColumn(\"dummy\", lit(\"dummy\"))\nsongDfChanged.show(false)\n\n//Save changed DataFrame\nsongDfChanged.write.mode(\"overwrite\").save(\"dbfs:/cs-spark-training/dummy/\")\n\n//Read in changed DataFrame\nspark.read.load(\"dbfs:/cs-spark-training/dummy/\").show(false)","user":"anonymous","dateUpdated":"2019-04-24T17:54:41+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121272110_1810633403","id":"20190424-175432_1311438676","dateCreated":"2019-04-24T17:54:32+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54779","dateFinished":"2019-04-24T17:54:41+0200","dateStarted":"2019-04-24T17:54:41+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:616)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\n  ... 51 elided\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121281598_1350708447","id":"20190424-175441_1870925159","dateCreated":"2019-04-24T17:54:41+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54868","text":"%md\n<b><font color='blue'size=5>Python</font>","dateUpdated":"2019-04-24T17:54:52+0200","dateFinished":"2019-04-24T17:54:52+0200","dateStarted":"2019-04-24T17:54:52+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\nfrom pyspark.sql.functions import *\n# Read in Parquet file\nsongDF = spark.read.load('dbfs:/cs-spark-training/Songs/')\nsongDF.show()\n\n# Operate on 'songDF'\nsongDFChanged = songDF.withColumn('dummy', lit('dummy'))\nsongDFChanged.show()\n\n# Save changed DataFrame\nsongDFChanged.write.mode(\"overwrite\").save('dbfs:/cs-spark-training/dummy/')\n\n# Read in changed DataFrame\nspark.read.load('dbfs:/cs-spark-training/dummy/').show()","user":"anonymous","dateUpdated":"2019-04-24T17:54:58+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121292750_-2069492496","id":"20190424-175452_314204927","dateCreated":"2019-04-24T17:54:52+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54966","dateFinished":"2019-04-24T17:54:58+0200","dateStarted":"2019-04-24T17:54:58+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'pyspark'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'pyspark'\n\n"}]}},{"text":"%md\n## 3.3.2 Saving RDDs\n\nText files can be saved with `.saveAsTextFile`\n\nOther file formats can be saved with `.saveAsHadoopFile`","user":"anonymous","dateUpdated":"2019-04-24T17:55:08+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121298126_-1969078223","id":"20190424-175458_1745640164","dateCreated":"2019-04-24T17:54:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55055","dateFinished":"2019-04-24T17:55:08+0200","dateStarted":"2019-04-24T17:55:08+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.3.2 Saving RDDs</h2>\n<p>Text files can be saved with <code>.saveAsTextFile</code></p>\n<p>Other file formats can be saved with <code>.saveAsHadoopFile</code></p>\n</div>"}]}},{"text":"%md\n### Example for saving a RDD as a text file","user":"anonymous","dateUpdated":"2019-04-24T17:55:15+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121308535_771205136","id":"20190424-175508_1786871836","dateCreated":"2019-04-24T17:55:08+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55144","dateFinished":"2019-04-24T17:55:15+0200","dateStarted":"2019-04-24T17:55:15+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for saving a RDD as a text file</h3>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:55:22+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121315382_-785173984","id":"20190424-175515_1145890268","dateCreated":"2019-04-24T17:55:15+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55234","dateFinished":"2019-04-24T17:55:22+0200","dateStarted":"2019-04-24T17:55:22+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"%spark\n//Read in a single CSV as text file\nval songRDD = sc.textFile(\"dbfs:/cs-spark-training/SongRDD/\")\nsongRDD.collect.foreach(println)\n\n//Operate on songRDD\nval wordRDD = songRDD.flatMap(x => x.split(\" \"))\n                     .filter(s => s == \"Muse\")\n\n//Save RDD\nwordRDD.saveAsTextFile(\"dbfs:/cs-spark-training/dummyRDD3/\")\n\nprintln(\"#######################################################\")\n//Load changed RDD\nsc.textFile(\"dbfs:/cs-spark-training/dummyRDD3/\").collect.foreach(println)\nprintln(\"#######################################################\")","user":"anonymous","dateUpdated":"2019-04-24T17:55:34+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121322959_1982265278","id":"20190424-175522_1750608235","dateCreated":"2019-04-24T17:55:22+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55324","dateFinished":"2019-04-24T17:55:34+0200","dateStarted":"2019-04-24T17:55:34+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n  ... 51 elided\n"}]}},{"text":"%md\n<b><font color='blue'size=5>Python</font>","user":"anonymous","dateUpdated":"2019-04-24T17:55:43+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121334231_-86471096","id":"20190424-175534_965033242","dateCreated":"2019-04-24T17:55:34+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55413","dateFinished":"2019-04-24T17:55:43+0200","dateStarted":"2019-04-24T17:55:43+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\n# Read in a single CSV as text file\nsongRDD = sc.textFile('dbfs:/cs-spark-training/SongRDD/')\nsongRDD.collect()\n\n# Operate on songRDD\nwordRDD = songRDD.flatMap(lambda x: x.split(' ')).filter(lambda x: x == 'Muse')\nwordRDD.collect()\n\n# Save RDD\nwordRDD.saveAsTextFile('dbfs:/cs-spark-training/dummyRDD/') # there is no options for overwriting for RDDS. hence, change foldername or delete folder beforehand\n\n# Load changed RDD\nsc.textFile('dbfs:/cs-spark-training/dummyRDD/').collect()","user":"anonymous","dateUpdated":"2019-04-24T17:55:48+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556121343214_927006119","id":"20190424-175543_622030628","dateCreated":"2019-04-24T17:55:43+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55511","dateFinished":"2019-04-24T17:55:48+0200","dateStarted":"2019-04-24T17:55:48+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\n"}]}}],"name":"/3. Ingestion & Saving/3.3 Saving","id":"2E97R5Q19","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}