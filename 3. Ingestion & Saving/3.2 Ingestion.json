{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119638903_2022807466","id":"20190424-172718_1736239905","dateCreated":"2019-04-24T17:27:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50019","text":"%md <img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>","dateUpdated":"2019-04-24T17:27:35+0200","dateFinished":"2019-04-24T17:27:35+0200","dateStarted":"2019-04-24T17:27:35+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src='https://global-uploads.webflow.com/5ad0acc69f356a98471287a3/5ae073d500595f83d49e713a_logo_Comsysto-Reply_color.svg' style='width:400px'>\n</div>"}]}},{"text":"%md\n# 3.2 Ingestion","user":"anonymous","dateUpdated":"2019-04-24T17:27:45+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119655313_-1720938561","id":"20190424-172735_1365411266","dateCreated":"2019-04-24T17:27:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50100","dateFinished":"2019-04-24T17:27:45+0200","dateStarted":"2019-04-24T17:27:45+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3.2 Ingestion</h1>\n</div>"}]}},{"text":"%md\n***3.2.1 DataFrame ingestion***\n\n* Common `.DataFrameReader` settings\n\n* Providing data source path\n\n* Examples for DataFrame ingestion out of a data source\n\n* Example for creating DataFrames out of in-memory data\n\n***3.2.2 Dataset ingestion***\n\n* Ingestion of text files with `.textFile`\n\n* Example for Dataset ingestion out of text files\n\n* Example for Dataset ingestion out of in-memory data\n\n***3.2.3 RDD ingestion***\n\n* Ingestion of a single text file with `.textFile`\n\n* Ingestion of multiple text files with `.wholeTextFile`\n\n* Example for RDD ingestion out of a single text file\n\n* Example for RDD ingestion of multiple text files\n\n* Example for RDD ingestion out of in-memory data","user":"anonymous","dateUpdated":"2019-04-24T17:27:52+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119661960_1218630001","id":"20190424-172741_1272823798","dateCreated":"2019-04-24T17:27:41+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50194","dateFinished":"2019-04-24T17:27:52+0200","dateStarted":"2019-04-24T17:27:52+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong><em>3.2.1 DataFrame ingestion</em></strong></p>\n<ul>\n  <li>\n  <p>Common <code>.DataFrameReader</code> settings</p></li>\n  <li>\n  <p>Providing data source path</p></li>\n  <li>\n  <p>Examples for DataFrame ingestion out of a data source</p></li>\n  <li>\n  <p>Example for creating DataFrames out of in-memory data</p></li>\n</ul>\n<p><strong><em>3.2.2 Dataset ingestion</em></strong></p>\n<ul>\n  <li>\n  <p>Ingestion of text files with <code>.textFile</code></p></li>\n  <li>\n  <p>Example for Dataset ingestion out of text files</p></li>\n  <li>\n  <p>Example for Dataset ingestion out of in-memory data</p></li>\n</ul>\n<p><strong><em>3.2.3 RDD ingestion</em></strong></p>\n<ul>\n  <li>\n  <p>Ingestion of a single text file with <code>.textFile</code></p></li>\n  <li>\n  <p>Ingestion of multiple text files with <code>.wholeTextFile</code></p></li>\n  <li>\n  <p>Example for RDD ingestion out of a single text file</p></li>\n  <li>\n  <p>Example for RDD ingestion of multiple text files</p></li>\n  <li>\n  <p>Example for RDD ingestion out of in-memory data</p></li>\n</ul>\n</div>"}]}},{"text":"%md\n## 3.2.1 DataFrame ingestion\n\nA DataFrame can be loaded with `spark.read`which creates a `DataFrameReader` object.\nWith the settings of the `DataFrameReader` it´s possbile to influence the process of reading in data.","user":"anonymous","dateUpdated":"2019-04-24T17:28:01+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119672104_-490633601","id":"20190424-172752_1781729318","dateCreated":"2019-04-24T17:27:52+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50307","dateFinished":"2019-04-24T17:28:01+0200","dateStarted":"2019-04-24T17:28:01+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.2.1 DataFrame ingestion</h2>\n<p>A DataFrame can be loaded with <code>spark.read</code>which creates a <code>DataFrameReader</code> object.<br/>With the settings of the <code>DataFrameReader</code> it´s possbile to influence the process of reading in data.</p>\n</div>"}]}},{"text":"%md\n### Common DataFrameReader settings\n\n*   `.format`: Specifies the type of the data source (e.g. CSV, JSON, Parquet, etc.)\n\n*   `.option`: Additional key-value settings (e.g. \"Header\",\"true\")\n\n*   `.schema`: Specifies the schema without inferencing\n\n*   `.load`:   For loading data out of files. `.load` without `.format` reads in Parquet as default.\n\n*   `.table`:  For loading data out of Hive tables\n\n*   `.csv`, `.json` etc: Instead of using `.load` one can use format specific short cuts.","user":"anonymous","dateUpdated":"2019-04-24T17:28:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119681520_-2049182188","id":"20190424-172801_7208871","dateCreated":"2019-04-24T17:28:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50397","dateFinished":"2019-04-24T17:28:14+0200","dateStarted":"2019-04-24T17:28:14+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Common DataFrameReader settings</h3>\n<ul>\n  <li>\n  <p><code>.format</code>: Specifies the type of the data source (e.g. CSV, JSON, Parquet, etc.)</p></li>\n  <li>\n  <p><code>.option</code>: Additional key-value settings (e.g. &ldquo;Header&rdquo;,&ldquo;true&rdquo;)</p></li>\n  <li>\n  <p><code>.schema</code>: Specifies the schema without inferencing</p></li>\n  <li>\n  <p><code>.load</code>: For loading data out of files. <code>.load</code> without <code>.format</code> reads in Parquet as default.</p></li>\n  <li>\n  <p><code>.table</code>: For loading data out of Hive tables</p></li>\n  <li>\n  <p><code>.csv</code>, <code>.json</code> etc: Instead of using <code>.load</code> one can use format specific short cuts.</p></li>\n</ul>\n</div>"}]}},{"text":"%md\n### Providing data source path\n\nThe data source path has to be provided inside `.load` respectively inside the format specific equivalents.\n\nYou can specify the path for...\n\n* **single files**      ->     spark.read.csv(\"oneFile.csv\")\n\n* **list of files** -> spark.read.csv(\"firstFile.csv\",\"secondFile.csv\")\n\n* **directories** -> spark.read.csv(\"directory/\")\n\n* **wildcard list of files** -> spark.read.csv(\"directory/*.csv\")","user":"anonymous","dateUpdated":"2019-04-24T17:28:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119691305_-986215385","id":"20190424-172811_1821630138","dateCreated":"2019-04-24T17:28:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50487","dateFinished":"2019-04-24T17:28:31+0200","dateStarted":"2019-04-24T17:28:31+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Providing data source path</h3>\n<p>The data source path has to be provided inside <code>.load</code> respectively inside the format specific equivalents.</p>\n<p>You can specify the path for&hellip;</p>\n<ul>\n  <li>\n  <p><strong>single files</strong> -&gt; spark.read.csv(&ldquo;oneFile.csv&rdquo;)</p></li>\n  <li>\n  <p><strong>list of files</strong> -&gt; spark.read.csv(&ldquo;firstFile.csv&rdquo;,&ldquo;secondFile.csv&rdquo;)</p></li>\n  <li>\n  <p><strong>directories</strong> -&gt; spark.read.csv(&ldquo;directory/&rdquo;)</p></li>\n  <li>\n  <p><strong>wildcard list of files</strong> -&gt; spark.read.csv(&ldquo;directory/*.csv&rdquo;)</p></li>\n</ul>\n</div>"}]}},{"text":"%md\n### Examples for DataFrame ingestions out of data sources","user":"anonymous","dateUpdated":"2019-04-24T17:28:40+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119711281_-190507194","id":"20190424-172831_1396428840","dateCreated":"2019-04-24T17:28:31+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50600","dateFinished":"2019-04-24T17:28:40+0200","dateStarted":"2019-04-24T17:28:40+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Examples for DataFrame ingestions out of data sources</h3>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:28:49+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119720581_1141826098","id":"20190424-172840_1914333283","dateCreated":"2019-04-24T17:28:40+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50690","dateFinished":"2019-04-24T17:28:49+0200","dateStarted":"2019-04-24T17:28:49+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"%spark\n//Read in Parquet file\nval songDF1 = spark.read\n                   .load(\"dbfs:/cs-spark-training/Songs/\")\nsongDF1.show(false)\n\n//is equivalent to\nval songDF2 = spark.read\n                    .format(\"parquet\")\n                    .load(\"dbfs:/cs-spark-training/Songs/\")\nsongDF2.show(false)\n\n\n\n//Read in CSV file and specifiy some DataFrameReader settings\nval songDF3 = spark.read\n                    .option(\"header\", true) //take first row as header\n                    .csv(\"dbfs:/cs-spark-training/SongDS/SongDS.csv\") //.csv is a format specific shortcut for `.load`\nsongDF3.show(false)\n\n//is equivalent to\nval songDF4 = spark.read\n                    .format(\"csv\")\n                    .option(\"header\", true) //take first row as header\n                    .load(\"dbfs:/cs-spark-training/SongDS/SongDS.csv\")\nsongDF4.show(false)","user":"anonymous","dateUpdated":"2019-04-24T17:29:05+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119729113_2120058296","id":"20190424-172849_1907259567","dateCreated":"2019-04-24T17:28:49+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50780","dateFinished":"2019-04-24T17:29:05+0200","dateStarted":"2019-04-24T17:29:05+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:616)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\n  ... 51 elided\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119735994_1290282412","id":"20190424-172855_1901339109","dateCreated":"2019-04-24T17:28:55+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:50870","text":"%md\n<b><font color='blue'size=5>Python</font>","dateUpdated":"2019-04-24T17:29:20+0200","dateFinished":"2019-04-24T17:29:20+0200","dateStarted":"2019-04-24T17:29:20+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\n# Read in Parquet file\nsongDF1 = spark.read.load('dbfs:/cs-spark-training/Songs/', format = 'parquet')\nsongDF1.show()\n\n# is equivalent to\nsongDF2 = spark.read.format('parquet').load('dbfs:/cs-spark-training/Songs/')\nsongDF2.show()\n\n# Read in CSV file and specifiy some DataFrameReader settings\nsongDF3 = spark.read.load('dbfs:/cs-spark-training/SongDS/SongDS.csv', header = True, format = 'csv')\nsongDF3.show()\n\n# is equivalent to\nsongDF4 = spark.read.format('csv').option('header', True).load('dbfs:/cs-spark-training/SongDS/SongDS.csv')\nsongDF4.show()","user":"anonymous","dateUpdated":"2019-04-24T17:29:42+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119760769_-1675203703","id":"20190424-172920_1325604960","dateCreated":"2019-04-24T17:29:20+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51008","dateFinished":"2019-04-24T17:29:42+0200","dateStarted":"2019-04-24T17:29:42+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'spark' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'spark' is not defined\n\n"}]}},{"text":"%md\n### Example for creating DataFrames out of in-memory data\n\nFor creation of DataFrames out of in-memory data one have to use `.createDataFrame`.","user":"anonymous","dateUpdated":"2019-04-24T17:29:59+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119782914_159756292","id":"20190424-172942_226744149","dateCreated":"2019-04-24T17:29:42+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51097","dateFinished":"2019-04-24T17:29:59+0200","dateStarted":"2019-04-24T17:29:59+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for creating DataFrames out of in-memory data</h3>\n<p>For creation of DataFrames out of in-memory data one have to use <code>.createDataFrame</code>.</p>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:30:09+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119799417_-1601801767","id":"20190424-172959_1239490018","dateCreated":"2019-04-24T17:29:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51186","dateFinished":"2019-04-24T17:30:09+0200","dateStarted":"2019-04-24T17:30:09+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"%spark\n//Manuell creation of a dataset\nval songSeq = Seq((\"Frank Sinatra\", -10.0),(\"Beastie Boys\", -5.0),(\"Muse\", -7.0))\n\n//Create DataFrame out of Sequence\nval songSeqDF = spark.createDataFrame(songSeq)\nsongSeqDF.show(false)","user":"anonymous","dateUpdated":"2019-04-24T17:30:20+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119809010_-1818402585","id":"20190424-173009_198178339","dateCreated":"2019-04-24T17:30:09+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51276","dateFinished":"2019-04-24T17:30:22+0200","dateStarted":"2019-04-24T17:30:20+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+-----+\n|_1           |_2   |\n+-------------+-----+\n|Frank Sinatra|-10.0|\n|Beastie Boys |-5.0 |\n|Muse         |-7.0 |\n+-------------+-----+\n\nsongSeq: Seq[(String, Double)] = List((Frank Sinatra,-10.0), (Beastie Boys,-5.0), (Muse,-7.0))\nsongSeqDF: org.apache.spark.sql.DataFrame = [_1: string, _2: double]\n"}]}},{"text":"%md\n<b><font color='blue'size=5>Python</font>","user":"anonymous","dateUpdated":"2019-04-24T17:30:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119820954_1438819710","id":"20190424-173020_1507656478","dateCreated":"2019-04-24T17:30:20+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51365","dateFinished":"2019-04-24T17:30:31+0200","dateStarted":"2019-04-24T17:30:31+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\nmyData = [('Frank Sinatra', -10.0),('Beastie Boys', -5.0),('Muse', -7.0)]\nsongDF = spark.createDataFrame(myData, ['artist', 'loudness'])\nsongDF.show()","user":"anonymous","dateUpdated":"2019-04-24T17:30:38+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119831249_-2006280392","id":"20190424-173031_1673304913","dateCreated":"2019-04-24T17:30:31+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51463","dateFinished":"2019-04-24T17:30:38+0200","dateStarted":"2019-04-24T17:30:38+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'spark' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'spark' is not defined\n\n"}]}},{"text":"%md\n## 3.2.2 Dataset ingestion\n\nTo create a Dataset directly out of a source file is only possible for ***text files***. In this case you have to call `spark.read.textFile(\"filePath\")`.\nThere is only one setting `.option(\"wholetext\", true)` applicable which is reading in the whole text into one single row. ","user":"anonymous","dateUpdated":"2019-04-24T17:30:53+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119838130_-1360376199","id":"20190424-173038_1889761016","dateCreated":"2019-04-24T17:30:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51552","dateFinished":"2019-04-24T17:30:53+0200","dateStarted":"2019-04-24T17:30:53+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.2.2 Dataset ingestion</h2>\n<p>To create a Dataset directly out of a source file is only possible for <strong><em>text files</em></strong>. In this case you have to call <code>spark.read.textFile(&quot;filePath&quot;)</code>.<br/>There is only one setting <code>.option(&quot;wholetext&quot;, true)</code> applicable which is reading in the whole text into one single row.</p>\n</div>"}]}},{"text":"%md\n### Example for Dataset ingestion out of text files","user":"anonymous","dateUpdated":"2019-04-24T17:31:00+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119853826_-1204571576","id":"20190424-173053_406144507","dateCreated":"2019-04-24T17:30:53+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51641","dateFinished":"2019-04-24T17:31:00+0200","dateStarted":"2019-04-24T17:31:00+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for Dataset ingestion out of text files</h3>\n</div>"}]}},{"text":"%spark\n//Create Dataset out of text file\nval songDS = spark.read.textFile(\"dbfs:/cs-spark-training/SongDS/SongDS.csv\")\nsongDS.show(false)\nsongDS.count()    //option \"wholetext\" by default is 'false'","user":"anonymous","dateUpdated":"2019-04-24T17:31:10+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119860850_1899936980","id":"20190424-173100_1713044707","dateCreated":"2019-04-24T17:31:00+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51731","dateFinished":"2019-04-24T17:31:10+0200","dateStarted":"2019-04-24T17:31:10+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:616)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:623)\n  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:657)\n  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:632)\n  ... 51 elided\n"}]}},{"text":"%spark\n//Create Dataset out of text file and apply option `wholetext`\nval songDS = spark.read.option(\"wholetext\", true).textFile(\"dbfs:/cs-spark-training/SongDS/SongDS.csv\")\nsongDS.show(false)\nsongDS.count()","user":"anonymous","dateUpdated":"2019-04-24T17:31:19+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119870274_-333176096","id":"20190424-173110_783984531","dateCreated":"2019-04-24T17:31:10+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51820","dateFinished":"2019-04-24T17:31:20+0200","dateStarted":"2019-04-24T17:31:19+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:616)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:623)\n  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:657)\n  at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:632)\n  ... 51 elided\n"}]}},{"text":"%md\n### Example for creating Datasets out of in-memory data\n\nFor creation of Datasets out of in-memory data one have to use `.createDataset`.","user":"anonymous","dateUpdated":"2019-04-24T17:31:35+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119879858_-736388014","id":"20190424-173119_1040379897","dateCreated":"2019-04-24T17:31:19+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:51917","dateFinished":"2019-04-24T17:31:35+0200","dateStarted":"2019-04-24T17:31:35+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for creating Datasets out of in-memory data</h3>\n<p>For creation of Datasets out of in-memory data one have to use <code>.createDataset</code>.</p>\n</div>"}]}},{"text":"%spark\n//Create DataFrame out of Sequence\nval songSeqDS = spark.createDataset(songSeq)\nsongSeqDS.show(false)","user":"anonymous","dateUpdated":"2019-04-24T17:31:45+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119895266_-232607096","id":"20190424-173135_1458684622","dateCreated":"2019-04-24T17:31:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52015","dateFinished":"2019-04-24T17:31:45+0200","dateStarted":"2019-04-24T17:31:45+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+-----+\n|_1           |_2   |\n+-------------+-----+\n|Frank Sinatra|-10.0|\n|Beastie Boys |-5.0 |\n|Muse         |-7.0 |\n+-------------+-----+\n\nsongSeqDS: org.apache.spark.sql.Dataset[(String, Double)] = [_1: string, _2: double]\n"}]}},{"text":"%md\n## 3.2.3 RDD ingestion\n\nText files can be loaded with a sparkConext `sc` followed by `.textFile()`.\n\nMulti-line text file can be loaded with a sparkContext `sc` followed by `.wholeTextFiles()`.\n\nIn addition it is possible to control the number of partitions with the 2nd parameter of both methods. For example `sc.textFile(\"path\", 5)` would distribute the data on 5 nodes. For DataFrames and Datasets you can´t control the number of partitions within ingestion.","user":"anonymous","dateUpdated":"2019-04-24T17:31:58+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119905490_-1677401444","id":"20190424-173145_1404496796","dateCreated":"2019-04-24T17:31:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52104","dateFinished":"2019-04-24T17:31:58+0200","dateStarted":"2019-04-24T17:31:58+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.2.3 RDD ingestion</h2>\n<p>Text files can be loaded with a sparkConext <code>sc</code> followed by <code>.textFile()</code>.</p>\n<p>Multi-line text file can be loaded with a sparkContext <code>sc</code> followed by <code>.wholeTextFiles()</code>.</p>\n<p>In addition it is possible to control the number of partitions with the 2nd parameter of both methods. For example <code>sc.textFile(&quot;path&quot;, 5)</code> would distribute the data on 5 nodes. For DataFrames and Datasets you can´t control the number of partitions within ingestion.</p>\n</div>"}]}},{"text":"%md\n### Ingestion of a single text file with `.textFile`\n\nMaps each row of the file into a individual RDD.\n\nApplicable for...\n\n* ***Single text files*** -> sc.textFile(\"oneFile.txt\")\n\n* ***Directory of text files*** -> sc.textFile(\"directory/\")\n\n* ***Wildcard list of text files*** -> sc.textFile(\"directory/*.txt\")\n\n* ***List of text files*** -> sc.textFile(\"firstTextFile.txt\",\"secondTextFile.txt\")\n\nApplicable only to newline-delimited text files.","user":"anonymous","dateUpdated":"2019-04-24T17:32:06+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119918266_877918636","id":"20190424-173158_1860143162","dateCreated":"2019-04-24T17:31:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52202","dateFinished":"2019-04-24T17:32:06+0200","dateStarted":"2019-04-24T17:32:06+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ingestion of a single text file with <code>.textFile</code></h3>\n<p>Maps each row of the file into a individual RDD.</p>\n<p>Applicable for&hellip;</p>\n<ul>\n  <li>\n  <p><strong><em>Single text files</em></strong> -&gt; sc.textFile(&ldquo;oneFile.txt&rdquo;)</p></li>\n  <li>\n  <p><strong><em>Directory of text files</em></strong> -&gt; sc.textFile(&ldquo;directory/&rdquo;)</p></li>\n  <li>\n  <p><strong><em>Wildcard list of text files</em></strong> -&gt; sc.textFile(&ldquo;directory/*.txt&rdquo;)</p></li>\n  <li>\n  <p><strong><em>List of text files</em></strong> -&gt; sc.textFile(&ldquo;firstTextFile.txt&rdquo;,&ldquo;secondTextFile.txt&rdquo;)</p></li>\n</ul>\n<p>Applicable only to newline-delimited text files.</p>\n</div>"}]}},{"text":"%md\n## Ingestion of multiple text files with `.wholeTextFiles`\n\nApplicable for multi-line file formats (e.i. XML, JSON, etc.)\n\nMaps whole content of each file in a RDD as a key-value pair of type RDD[(String, String)].","user":"anonymous","dateUpdated":"2019-04-24T17:32:16+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119926458_850902193","id":"20190424-173206_913586574","dateCreated":"2019-04-24T17:32:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52292","dateFinished":"2019-04-24T17:32:16+0200","dateStarted":"2019-04-24T17:32:16+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Ingestion of multiple text files with <code>.wholeTextFiles</code></h2>\n<p>Applicable for multi-line file formats (e.i. XML, JSON, etc.)</p>\n<p>Maps whole content of each file in a RDD as a key-value pair of type RDD[(String, String)].</p>\n</div>"}]}},{"text":"%md\n### Example for RDD ingestion out of a single text file","user":"anonymous","dateUpdated":"2019-04-24T17:32:27+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119936827_-1810101435","id":"20190424-173216_264296860","dateCreated":"2019-04-24T17:32:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52382","dateFinished":"2019-04-24T17:32:27+0200","dateStarted":"2019-04-24T17:32:27+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for RDD ingestion out of a single text file</h3>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:32:35+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119947419_-456721598","id":"20190424-173227_943732525","dateCreated":"2019-04-24T17:32:27+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52472","dateFinished":"2019-04-24T17:32:35+0200","dateStarted":"2019-04-24T17:32:35+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"%spark\n//Read in a single CSV as text file\nval songRDD = sc.textFile(\"dbfs:/cs-spark-training/SongDS/SongDS.csv\")\n\nsongRDD.collect.foreach(println)","user":"anonymous","dateUpdated":"2019-04-24T17:32:45+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119955937_242938585","id":"20190424-173235_1792421813","dateCreated":"2019-04-24T17:32:35+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52562","dateFinished":"2019-04-24T17:32:46+0200","dateStarted":"2019-04-24T17:32:45+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n  ... 51 elided\n"}]}},{"text":"%md\n<b><font color='blue'size=5>Python</font>","user":"anonymous","dateUpdated":"2019-04-24T17:32:54+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119965866_-2131631868","id":"20190424-173245_2094772055","dateCreated":"2019-04-24T17:32:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52651","dateFinished":"2019-04-24T17:32:54+0200","dateStarted":"2019-04-24T17:32:54+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\n# note the header\nsongRDD = sc.textFile('dbfs:/cs-spark-training/SongDS/SongDS.csv')\nsongRDD.collect()","user":"anonymous","dateUpdated":"2019-04-24T17:33:01+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119974323_1564401356","id":"20190424-173254_436182895","dateCreated":"2019-04-24T17:32:54+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52749","dateFinished":"2019-04-24T17:33:01+0200","dateStarted":"2019-04-24T17:33:01+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\n"}]}},{"text":"%md\n### Example for RDD ingestion of multiple text files","user":"anonymous","dateUpdated":"2019-04-24T17:33:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119981091_-1934072579","id":"20190424-173301_295908340","dateCreated":"2019-04-24T17:33:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52838","dateFinished":"2019-04-24T17:33:14+0200","dateStarted":"2019-04-24T17:33:14+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for RDD ingestion of multiple text files</h3>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:33:22+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556119994523_-788415193","id":"20190424-173314_497060504","dateCreated":"2019-04-24T17:33:14+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52927","dateFinished":"2019-04-24T17:33:22+0200","dateStarted":"2019-04-24T17:33:22+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"%spark\nval multipleFileRDD = sc.wholeTextFiles(\"dbfs:/cs-spark-training/textDirectory/*\")\n\nmultipleFileRDD.collect.foreach(println)","user":"anonymous","dateUpdated":"2019-04-24T17:33:32+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120002618_681843324","id":"20190424-173322_1132184287","dateCreated":"2019-04-24T17:33:22+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53017","dateFinished":"2019-04-24T17:33:32+0200","dateStarted":"2019-04-24T17:33:32+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: No FileSystem for scheme: dbfs\n  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469)\n  at org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:877)\n  at org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:872)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.SparkContext.withScope(SparkContext.scala:701)\n  at org.apache.spark.SparkContext.wholeTextFiles(SparkContext.scala:872)\n  ... 51 elided\n"}]}},{"text":"%md\n<b><font color='blue'size=5>Python</font>","user":"anonymous","dateUpdated":"2019-04-24T17:33:44+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120012323_1512336289","id":"20190424-173332_857003921","dateCreated":"2019-04-24T17:33:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53106","dateFinished":"2019-04-24T17:33:44+0200","dateStarted":"2019-04-24T17:33:44+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\nmultipleFileRDD = sc.wholeTextFiles('dbfs:/cs-spark-training/textDirectory/*')\nmultipleFileRDD.collect()","user":"anonymous","dateUpdated":"2019-04-24T17:33:55+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120024459_-224757924","id":"20190424-173344_2069309779","dateCreated":"2019-04-24T17:33:44+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53204","dateFinished":"2019-04-24T17:33:55+0200","dateStarted":"2019-04-24T17:33:55+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\n"}]}},{"text":"%md\n### Example for RDD ingestion out of in-memory data\n\nFor creation of RDDs out of in-memory data one have to use `.parallelize`.","user":"anonymous","dateUpdated":"2019-04-24T17:34:06+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120035659_456160928","id":"20190424-173355_1142755991","dateCreated":"2019-04-24T17:33:55+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53293","dateFinished":"2019-04-24T17:34:06+0200","dateStarted":"2019-04-24T17:34:06+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example for RDD ingestion out of in-memory data</h3>\n<p>For creation of RDDs out of in-memory data one have to use <code>.parallelize</code>.</p>\n</div>"}]}},{"text":"%md\n<b><font color='vermilion'size=5>Scala</font>","user":"anonymous","dateUpdated":"2019-04-24T17:34:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120046483_2045707241","id":"20190424-173406_1475455515","dateCreated":"2019-04-24T17:34:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53382","dateFinished":"2019-04-24T17:34:14+0200","dateStarted":"2019-04-24T17:34:14+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='vermilion'size=5>Scala</font></p>\n</div>"}]}},{"text":"%spark\n//Parallelize 'songSeq'\nval songRDDSeq = sc.parallelize(songSeq)\n\nsongRDDSeq.collect.foreach(println)","user":"anonymous","dateUpdated":"2019-04-24T17:34:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120054883_-564312383","id":"20190424-173414_1941438649","dateCreated":"2019-04-24T17:34:14+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53472","dateFinished":"2019-04-24T17:34:31+0200","dateStarted":"2019-04-24T17:34:31+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(Frank Sinatra,-10.0)\n(Beastie Boys,-5.0)\n(Muse,-7.0)\nsongRDDSeq: org.apache.spark.rdd.RDD[(String, Double)] = ParallelCollectionRDD[53] at parallelize at <console>:34\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.178.200:4040/jobs/job?id=28"],"interpreterSettingId":"spark"}}},{"text":"%md\n<b><font color='blue'size=5>Python</font>","user":"anonymous","dateUpdated":"2019-04-24T17:34:42+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120071714_1568964329","id":"20190424-173431_767871192","dateCreated":"2019-04-24T17:34:31+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53561","dateFinished":"2019-04-24T17:34:42+0200","dateStarted":"2019-04-24T17:34:42+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><b><font color='blue'size=5>Python</font></p>\n</div>"}]}},{"text":"%python\nsongRDD = sc.parallelize(myData)\nsongRDD.collect()","user":"anonymous","dateUpdated":"2019-04-24T17:34:48+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1556120082922_-99579116","id":"20190424-173442_836599914","dateCreated":"2019-04-24T17:34:42+0200","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53675","dateFinished":"2019-04-24T17:34:48+0200","dateStarted":"2019-04-24T17:34:48+0200","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 319, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_python-9202459694796226681.py\", line 307, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'sc' is not defined\n\n"}]}}],"name":"/3. Ingestion & Saving/3.2 Ingestion","id":"2EBKDTCNX","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}